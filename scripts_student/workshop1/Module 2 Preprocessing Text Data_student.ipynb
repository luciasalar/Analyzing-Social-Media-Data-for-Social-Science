{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "344dbfb4",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "In this module, you will learn how to do data cleaning:\n",
    "\n",
    "1. Remove punctuations\n",
    "2. Lowering text\n",
    "3. Removing URLs \n",
    "4. Tokenization (Chinese)\n",
    "5. Remove stop words\n",
    "6. Stemming\n",
    "7. Replacing emoji with text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ce2e3",
   "metadata": {},
   "source": [
    "# Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39582af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8bb5b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation   #these are the punctuations contained in the library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a1dca6",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce28ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The cat~ says meow..!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec515dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat says meow\n"
     ]
    }
   ],
   "source": [
    "punctuation_free = []\n",
    "for character in sentence:\n",
    "    if character not in string.punctuation:\n",
    "        punctuation_free.append(character)\n",
    "        no_pun_sentence = \"\".join(punctuation_free) \n",
    "       \n",
    "print(no_pun_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fab8ce9",
   "metadata": {},
   "source": [
    "## Alternatively, you can write this in one line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac7bef97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat says meow\n"
     ]
    }
   ],
   "source": [
    "no_pun_sentence = \"\".join([character for character in sentence if character not in string.punctuation]) \n",
    "       \n",
    "print(no_pun_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3853618",
   "metadata": {},
   "source": [
    "## Task 1: Remove punctuations in the tweets\n",
    "\n",
    "hint: \n",
    "\n",
    "access each tweet element in the dictionary, identify elements that are not in the string library\n",
    "\n",
    "join the elements using \"\".join(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb7cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict = {\n",
    "  1: {\"userid\": \"000234\",\n",
    "  \"Tweet\": \"That strange moment when someone reminds the teacher about the homework.\"},\n",
    "  2: {\"userid\": \"002214\",\n",
    "  \"Tweet\": \"Hey, I get really nervous when I see others studying so much before the test.!!\"},     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e0f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "514a748f",
   "metadata": {},
   "source": [
    "# Lowering text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "48f70701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat~ says meow..!!'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The cat~ says meow..!!\"\n",
    "sentence.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d9988f",
   "metadata": {},
   "source": [
    "## Task 2: Remove punctuations in the tweets (dictionary structure)\n",
    "\n",
    "using tweet_dict as an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38ed5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee409018",
   "metadata": {},
   "source": [
    "# Remove URLs\n",
    "\n",
    "\n",
    "Regular expression syntax let you check if a particular string matches a given regular expression \n",
    "\n",
    "Here's a quick start for you https://www.rexegg.com/regex-quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea9b63fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "16338f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_url = \"The course materials are on https://lushichen.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca233bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The course materials are on '"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"http\\S+\", \"\", sentence_url) # means substitute \"http+a string of non-whitespace characters\" with \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e2ed0",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4442ea4",
   "metadata": {},
   "source": [
    "We already learnt the English tokenization in the previous module, task 7: split words in sentence\n",
    "\n",
    "In some languages, the boundary of a word is not separated by space, we need some packages to help with tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec3f2a",
   "metadata": {},
   "source": [
    "## Tokenizing Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831b313f",
   "metadata": {},
   "source": [
    "jieba: \"Jieba\" (Chinese for \"to stutter\") Chinese text segmentation\n",
    "\n",
    "pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b4692160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f358ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.add_word('äºŽå‰', freq=None, tag='nr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "92478755",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "èµµå°åˆš nr\n",
      "æ›¾ç» d\n",
      "é‡‡è®¿ v\n",
      "è¿‡ ug\n",
      "ä¸€ä½ m\n",
      "æ„å¤§åˆ© ns\n",
      "åŽ¨å¸ˆ n\n",
      "ï¼Œ x\n",
      "ä»– r\n",
      "åå‡ å¹´ m\n",
      "å‰æ¥ n\n",
      "è¿‡ ug\n",
      "ä¸­å›½ ns\n",
      "ï¼Œ x\n",
      "åŽ» v\n",
      "äº† ul\n",
      "ä¸å°‘ d\n",
      "åœ°æ–¹ n\n",
      "ï¼Œ x\n",
      "ä»– r\n",
      "å–œæ¬¢ v\n",
      "å·èœ n\n",
      "ï¼Œ x\n",
      "å¯¹ p\n",
      "ç²¤èœ n\n",
      "ä¹Ÿ d\n",
      "é¢‡ä¸º v\n",
      "ä¸Šç˜¾ v\n",
      "ï¼Œ x\n",
      "ä»– r\n",
      "å¥½å¥‡ a\n",
      "ä¸­é¤ n\n",
      "è¿™ä¹ˆ r\n",
      "å¥½åƒ v\n",
      "ï¼Œ x\n",
      "é«˜çº§ b\n",
      "çš„ uj\n",
      "é¥­é¦† n\n",
      "ä¸ºä»€ä¹ˆ r\n",
      "è¿™ä¹ˆ r\n",
      "å°‘ a\n"
     ]
    }
   ],
   "source": [
    "text = \"èµµå°åˆšæ›¾ç»é‡‡è®¿è¿‡ä¸€ä½æ„å¤§åˆ©åŽ¨å¸ˆï¼Œä»–åå‡ å¹´å‰æ¥è¿‡ä¸­å›½ï¼ŒåŽ»äº†ä¸å°‘åœ°æ–¹ï¼Œä»–å–œæ¬¢å·èœï¼Œå¯¹ç²¤èœä¹Ÿé¢‡ä¸ºä¸Šç˜¾ï¼Œä»–å¥½å¥‡ä¸­é¤è¿™ä¹ˆå¥½åƒï¼Œé«˜çº§çš„é¥­é¦†ä¸ºä»€ä¹ˆè¿™ä¹ˆå°‘\"\n",
    "words = pseg.cut(text)\n",
    "for w in words:\n",
    "    print('%s %s' % (w.word, w.flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2685e18f",
   "metadata": {},
   "source": [
    "## Customize word boundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3e3dd5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3001"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.suggest_freq('å‰æ¥', tune=True)  # you want to reduce this word frequency so that jieba doesn't cut it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "99122c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.add_word('å‰æ¥', freq=10, tag='n') #adjust frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "95bbaf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä»– r\n",
      "åå‡ å¹´ m\n",
      "å‰ f\n",
      "æ¥è¿‡ n\n",
      "ä¸­å›½ ns\n"
     ]
    }
   ],
   "source": [
    "text = \"ä»–åå‡ å¹´å‰æ¥è¿‡ä¸­å›½\"\n",
    "words = pseg.cut(text)\n",
    "for w in words:\n",
    "    print('%s %s' % (w.word, w.flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ffcbc",
   "metadata": {},
   "source": [
    "# Remove stop words\n",
    "\n",
    "install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d392c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b7738bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/luciachen/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f2fb4cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stop words present in the library\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c6f10449",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hey, I get really nervous when I see others studying so much before the test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0214077",
   "metadata": {},
   "source": [
    "#### the stopword dictionary is lower case, we need to lower case the sentence before we search for stop words in the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ac2246e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey,', 'get', 'really', 'nervous', 'see', 'others', 'studying', 'much', 'test']\n"
     ]
    }
   ],
   "source": [
    "output= [i for i in sentence.lower().split(' ') if i not in stopwords]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b992b",
   "metadata": {},
   "source": [
    "## Task 2:  Customize the stopword list in nltk\n",
    "\n",
    "we want to remove the word 'hey' from the sentence, add this word to nltk stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b606d5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65022c0c",
   "metadata": {},
   "source": [
    "# Stemming \n",
    "\n",
    "taking the root/base form of the word, for example, 'programming' -> \"program\", \"speaks\" -> \"speak\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d6e89220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "65d84655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the object for stemming\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a0f39a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey,', 'i', 'get', 'realli', 'nervou', 'when', 'i', 'see', 'other', 'studi', 'so', 'much', 'befor', 'the', 'test']\n"
     ]
    }
   ],
   "source": [
    "stem_text = [porter_stemmer.stem(word) for word in sentence.split(' ')]\n",
    "print(stem_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a4396",
   "metadata": {},
   "source": [
    "# Replace emmoji with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "59d0d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9d918d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'game is on fire fire'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"game is on ðŸ”¥ ðŸ”¥\"\n",
    "emoji.demojize(text, delimiters=(\"\", \"\"))  # 'game is on fire fire'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b9598",
   "metadata": {},
   "source": [
    "# Task 3: Write a function to preprocess text \n",
    "\n",
    "Joining all the previous steps into one function\n",
    "\n",
    "Here's how you define a function in Python\n",
    "\n",
    "def my_function(arg1, arg2):\n",
    "\n",
    "    do something\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "acd1ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"hey, I want to organize a BBQ tonight ðŸ”¥ ðŸ”¥, do you want to join?\"\n",
    "\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5870168",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a2ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa96c995",
   "metadata": {},
   "source": [
    "# Task 4: Preprocess tweets in dictionary structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7b6ef666",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict = {\n",
    "  1: {\"userid\": \"000234\",\n",
    "  \"Tweet\": \"That strange moment when someone reminds the teacher about the homework.\"},\n",
    "  2: {\"userid\": \"002214\",\n",
    "  \"Tweet\": \"Hey, I get really nervous when I see others studying so much before the test.ðŸ”¥ ðŸ”¥!!\"},     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "573348cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(tweets):\n",
    "    \n",
    "        \n",
    "    return new_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03edd0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1295a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e4223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
